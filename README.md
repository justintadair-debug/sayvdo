# Say vs. Do â€” Corporate Truthfulness Engine

> **We score how honest public companies are between what they say and what they actually do.**

Public companies make promises in earnings calls, annual reports, and proxy statements. Then they do (or don't do) those things. We track the gap.

---

## What It Does

Say vs. Do analyzes SEC filings and scores companies across **5 dimensions of corporate honesty**:

| Dimension | What We Measure |
|---|---|
| **AI Disclosure** | Are AI claims specific, measurable, and verifiable â€” or just buzzwords? |
| **Guidance Accuracy** | How close were their forward-looking statements to actual results? |
| **Risk Language Drift** | Are risk disclosures getting vaguer over time (a red flag)? |
| **Capital Honesty** | Do capital allocation decisions match stated priorities? |
| **ESG Substance** | Is ESG reporting backed by real commitments or just PR? |

---

## Sample Scores

| Company | AI Disclosure | Overall |
|---|---|---|
| NVDA | 94 / 100 | ðŸŸ¢ Genuine |
| GOOG | 84 / 100 | ðŸŸ¢ Genuine |
| MSFT | 82 / 100 | ðŸŸ¢ Genuine |

---

## How It Works

1. **Data Ingestion** â€” Pulls 10-K, 8-K, and DEF14A filings from EDGAR (SEC's public database)
2. **AI Analysis** â€” Claude reads and scores each filing across the 5 dimensions
3. **Tracking** â€” Scores are stored over time so you can see a company's honesty trend
4. **API** â€” Results served via FastAPI on port 8095

```
GET /score/{ticker}         â†’ Latest truthfulness scores
GET /history/{ticker}       â†’ Historical score trend
GET /leaderboard            â†’ Top and bottom companies by dimension
```

---

## Tech Stack

- **Python** â€” Core analysis pipeline
- **FastAPI** â€” REST API (port 8095)
- **SQLite** â€” Score storage and historical tracking
- **EDGAR API** â€” SEC filing retrieval (no API key required)
- **Claude AI** â€” Multi-dimensional filing analysis

---

## Status

ðŸŸ¢ **Live** â€” Actively scoring S&P 500 companies

---

*Built by Justin Adair | Part of the Neo AI portfolio*
